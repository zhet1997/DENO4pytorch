Transformer_config:
#  in_dim: 363
  node_feats: 28
  pos_dim: 2
  n_targets: 5
  n_hidden: 256
  num_feat_layers: 0
  num_encoder_layers: 5
  n_head: 2
  dim_feedforward: 128
  attention_type: galerkin
  feat_extract_type: None
  xavier_init: 0.01
  diagonal_weight: 0.01
  symmetric_init: False
  layer_norm: True
  attn_norm: False
  norm_eps: 0.0000001
  batch_norm: False
  return_attn_weight: False
  return_latent: False
  decoder_type: ifft2
  last_activation: False
  freq_dim: 64
  num_regressor_layers: 4
  regressor_activation: gelu
  fourier_modes: 8
  spacial_dim: 2
  spacial_fc: True
  dropout: 0.5
  encoder_dropout: 0.0
  decoder_dropout: 0.0
  ffn_dropout: 0.00
  debug: False
  upsample_mode: interp
  downsample_mode: interp
  boundary_condition: dirichlet

TNO_config:
  in_dim: 28
  out_dim: 5

MLP_config:
  in_dim: 28
  out_dim: 20480
  is_BatchNorm: True
  n_hidden: 256
  num_layers: 8

FNO_config:
  activation: relu
  depth: 6
  in_dim: 28
  modes: 6
  out_dim: 5
  padding: 8
  steps: 1
  width: 128
  
deepONet_config:
  input_dim: 2
  operator_dims: [28]
  output_dim: 5
  planes_branch: [128, 128, 128, 128]
  planes_trunk: [128, 128, 128, 128]

UNet_config:
  in_sizes: [64, 64, 28]
  out_sizes: [64, 64, 5]
  width: 64
  depth: 4
  steps: 1
  activation: 'gelu'
  dropout: 0

Optimizer_config:
  learning_rate: 0.001

Scheduler_config:
  learning_rate: 0.001
  step_size: 800
  gamma: 0.1

Basic_config:
  batch_size: 32
  ntrain: 2500
  nvalid: 400
  noise_scale: 0
  shuffled: True

